# Pipeline Orchestration

This document describes the **execution flow** of the Abridge pipeline as implemented in `run_pipeline.py`.

It specifies the exact sequence of operations, stage dependencies, skip validation, and error handling behavior derived directly from the codebase.

---

## 1. Overview

The pipeline orchestrator (`run_pipeline.py`) coordinates:
- Three REQUIRED condensation stages (chapter → arc → novel)
- Six OPTIONAL feature stages (Tier-2 and Tier-3)
- Observational instrumentation (guardrails, cost tracking, run reports)

**Entry Point:**
```bash
python run_pipeline.py <novel_name> [flags]
```

---

## 2. Execution Sequence

The pipeline executes in this exact order:

```
1. Initialize run context (run_id, metadata)
2. [Stage 1] Chapter Condensation (or skip if --skip-chapters)
3. [Stage 2] Arc Condensation (or skip if --skip-arcs)
4. [Stage 3] Novel Condensation (or skip if --skip-novel)
5. [Tier-2] Character Index (if --character-index)
6. [Tier-3.1] Character Salience (if --character-salience)
7. [Tier-3.2] Relationship Matrix (if --relationship-matrix)
8. [Tier-3.3] Event Keywords (if --event-keywords)
9. [Tier-3.4a] Genre Resolver (if --genre-resolver)
10. [Tier-3.4b] Tag Resolver (if --tag-resolver)
11. [Finally] Print guardrail summary
12. [Finally] Print cost summary
13. [Finally] Generate and save run report
14. [Finally] End run context
```

**Critical:** Steps 11-14 execute in a `finally` block and run even if earlier stages fail.

---

## 3. Run Context Management

### Run ID Generation

```python
run_id = f"run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
```

Format: `run_YYYYMMDD_HHMMSS_XXXXXXXX`

### Run Metadata

The `RunMetadata` dataclass collects:
- `run_id`: Unique identifier
- `novel_name`: Target novel
- `start_time`: Pipeline start timestamp
- `end_time`: Pipeline end timestamp
- `llm_provider`: Provider name from config
- `model_name`: Model name for provider
- `chapters_skipped`, `arcs_skipped`, `novel_skipped`: Skip status
- `chapters_count`, `arcs_count`: Output counts

---

## 4. Stage Configuration

### Required Directories

| Constant | Value | Purpose |
|----------|-------|---------|
| `RAW_DIR` | `data/raw` | Input chapters |
| `CHAPTERS_CONDENSED_DIR` | `data/chapters_condensed` | Stage 1 output |
| `ARCS_CONDENSED_DIR` | `data/arcs_condensed` | Stage 2 output |
| `NOVEL_CONDENSED_DIR` | `data/novel_condensed` | Stage 3 output |

### SkipFlags Dataclass

```python
@dataclass
class SkipFlags:
    skip_chapters: bool = False
    skip_arcs: bool = False
    skip_novel: bool = False
    character_index: bool = False
    character_salience: bool = False
    relationship_matrix: bool = False
    event_keywords: bool = False
    genre_resolver: bool = False
    tag_resolver: bool = False
```

---

## 5. Skip Validation

Each condensation stage has explicit validation before skip is allowed.

### Chapter Skip Validation (`validate_chapter_outputs`)

**Checks:**
1. Raw directory exists: `data/raw/{novel_name}/`
2. Output directory exists: `data/chapters_condensed/{novel_name}/`
3. Raw chapter count > 0
4. Condensed chapter count > 0
5. **Condensed count == Raw count** (strict 1:1 mapping)

**On validation failure:** Raises `ValueError` with message explaining the issue.

### Arc Skip Validation (`validate_arc_outputs`)

**Checks:**
1. Output directory exists: `data/arcs_condensed/{novel_name}/`
2. At least one arc file present (count > 0)

**Note:** Arc count is not validated against expected count because `CHAPTERS_PER_ARC` may vary.

### Novel Skip Validation (`validate_novel_outputs`)

**Checks:**
1. Output directory exists: `data/novel_condensed/{novel_name}/`
2. File exists: `data/novel_condensed/{novel_name}/novel.condensed.txt`
3. File is non-empty (size > 0)

---

## 6. Stage Execution

### Stage 1: Chapter Condensation

```python
if skip_flags.skip_chapters:
    is_valid, message = validate_chapter_outputs(novel_name)
    if is_valid:
        metadata.chapters_skipped = True
    else:
        raise ValueError(...)
else:
    condense_chapters(novel_name)
```

**Function called:** `chapter_condensation.process_novel(novel_name)`

### Stage 2: Arc Condensation

```python
if skip_flags.skip_arcs:
    is_valid, message = validate_arc_outputs(novel_name)
    if is_valid:
        metadata.arcs_skipped = True
    else:
        raise ValueError(...)
else:
    condense_arcs(novel_name)
```

**Function called:** `arc_condensation.process_novel(novel_name)`

### Stage 3: Novel Condensation

```python
if skip_flags.skip_novel:
    is_valid, message = validate_novel_outputs(novel_name)
    if is_valid:
        metadata.novel_skipped = True
    else:
        raise ValueError(...)
else:
    condense_novel(novel_name)
```

**Function called:** `novel_condensation.process_novel(novel_name)`

---

## 7. Tier-2/3 Feature Execution

Tier-2 and Tier-3 features are OPTIONAL and require explicit flags.

### Tier-2: Character Index

```python
if skip_flags.character_index:
    generate_character_index(novel_name, run_id)
```

**Non-blocking:** Failures logged but do not halt pipeline.

### Tier-3.1: Character Salience

```python
if skip_flags.character_salience:
    generate_salience_index(novel_name, run_id)
```

**Dependency:** Requires Tier-2 character index to exist.

### Tier-3.2: Relationship Matrix

```python
if skip_flags.relationship_matrix:
    generate_relationship_matrix(novel_name, run_id)
```

**Dependencies:** Requires Tier-2 and Tier-3.1 data.

### Tier-3.3: Event Keywords

```python
if skip_flags.event_keywords:
    generate_event_keyword_map(novel_name, run_id)
```

**Dependencies:** None (operates directly on condensed chapters).

### Tier-3.4a: Genre Resolver

```python
if skip_flags.genre_resolver:
    generate_genre_resolved(novel_name, run_id)
```

**Dependencies:** Tier-3.1, Tier-3.2, Tier-3.3 (works best with all three).

### Tier-3.4b: Tag Resolver

```python
if skip_flags.tag_resolver:
    generate_tag_resolved(novel_name, run_id)
```

**Dependencies:** Tier-3.1 through Tier-3.4a.

---

## 8. Observational Instrumentation

### Guardrails

- `start_run()`: Called at pipeline start, returns `run_id`
- `record_condensation()`: Called by each condensation stage
- `print_run_summary(run_id)`: Called in finally block
- `end_run()`: Called in finally block

### Cost Tracking

- `record_llm_usage()`: Called by each LLM invocation
- `print_usage_summary(run_id)`: Called in finally block

### Run Reports

```python
finalized_metadata = finalize_run_metadata()
generate_and_save_report(run_id, novel_name, finalized_metadata)
clear_run_metadata()
```

**Non-blocking:** Report generation errors are logged but do not halt.

---

## 9. Error Handling

### Stage Failures

- LLM failures: Retried up to `MAX_LLM_RETRIES=3` times per unit
- All retries exhausted: `RuntimeError` raised, pipeline halts
- Skip validation failures: `ValueError` raised, pipeline halts

### Tier-2/3 Failures

- Feature generation failures: Logged, pipeline continues
- Missing dependency data: Logged, feature skipped

### Finally Block Guarantee

The following always execute regardless of success/failure:
1. `print_run_summary(run_id)`
2. `print_usage_summary(run_id)`
3. `generate_and_save_report(...)`
4. `clear_run_metadata()`
5. `end_run()`

---

## 10. CLI Reference

```
usage: run_pipeline.py [-h] [--skip-chapters] [--skip-arcs] [--skip-novel]
                       [--character-index] [--character-salience]
                       [--relationship-matrix] [--event-keywords]
                       [--genre-resolver] [--tag-resolver]
                       novel_name

positional arguments:
  novel_name           Name of the novel (subdirectory under data/raw/)

optional arguments:
  --skip-chapters      Skip chapter condensation, reuse existing
  --skip-arcs          Skip arc condensation, reuse existing
  --skip-novel         Skip novel condensation, reuse existing
  --character-index    Generate character surface index (Tier-2)
  --character-salience Compute character salience scores (Tier-3.1)
  --relationship-matrix Compute character pair co-presence signals (Tier-3.2)
  --event-keywords     Scan for event keyword signals (Tier-3.3)
  --genre-resolver     Resolve genres from Tier-3 evidence (Tier-3.4a)
  --tag-resolver       Resolve tags from Tier-3 evidence (Tier-3.4b)
```

---

## 11. Example Invocations

### Full Pipeline (Default)

```bash
python run_pipeline.py "Heaven Reincarnation"
```

Executes: Stages 1, 2, 3 only.

### Resume from Arc Condensation

```bash
python run_pipeline.py "Heaven Reincarnation" --skip-chapters
```

Skips: Stage 1 (requires valid chapter outputs)
Executes: Stages 2, 3

### Full Feature Extraction

```bash
python run_pipeline.py "Heaven Reincarnation" \
  --skip-chapters --skip-arcs --skip-novel \
  --character-index \
  --character-salience \
  --relationship-matrix \
  --event-keywords \
  --genre-resolver \
  --tag-resolver
```

Skips: Stages 1, 2, 3 (requires all valid outputs)
Executes: Tier-2, Tier-3.1, Tier-3.2, Tier-3.3, Tier-3.4a, Tier-3.4b

---

## 12. Idempotency Notes

**Condensation stages:**
- Resume detection skips already-processed units
- Never overwrites existing valid outputs
- Empty outputs treated as corrupted and reprocessed

**Tier-2/3 features:**
- Outputs keyed by `run_id`
- Each run produces new artifacts
- Previous run artifacts are never overwritten
