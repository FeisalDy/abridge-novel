# Analysis-First Pipeline Design

This document describes the **design philosophy** and **implementation details** of the analysis-first orchestrator (`run_analysis_pipeline.py`).

---

## 1. Design Philosophy

### 1.1 The Problem with Condensation-First

The original `run_pipeline.py` assumes:
1. **Condensation is the goal** - You want a shortened novel
2. **Analysis is supplementary** - Tier-2/3 features are "nice to have"
3. **LLM costs are acceptable** - You're willing to pay for condensation

But many use cases DON'T require condensation:
- **Metadata extraction** - Genre/tag classification from existing text
- **Character network analysis** - Understanding relationships without rewriting
- **Keyword scanning** - Finding thematic elements without summarization
- **Prototype testing** - Validating analysis modules before expensive runs

### 1.2 The Analysis-First Paradigm

The analysis-first pipeline inverts the priorities:

```
CONDENSATION-FIRST (run_pipeline.py):
  Condensation (required) --> Analysis (optional)
  "Condense first, analyze if you want"

ANALYSIS-FIRST (run_analysis_pipeline.py):
  Analysis (required) --> Condensation (optional)
  "Analyze first, condense if you need"
```

### 1.3 Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| Analysis ON by default | Primary use case is metadata extraction |
| Condensation OFF by default | Avoid accidental LLM costs |
| RAW chapters supported | Don't require preprocessing |
| Auto-detect data source | Convenient when both exist |
| Non-breaking module changes | Backward compatible with original pipeline |

---

## 2. Data Source Architecture

### 2.1 Dual-Source Support

The analysis pipeline can read from TWO sources:

```
data/
  raw/                    <-- SOURCE 1: Original chapters
    {novel_name}/
      chapter_001.txt
      chapter_002.txt
      ...
  
  chapters_condensed/     <-- SOURCE 2: Condensed chapters
    {novel_name}/
      chapter_001.condensed.txt
      chapter_002.condensed.txt
      ...
```

### 2.2 Source Selection Logic

```python
def determine_data_source(novel_name, prefer_raw, prefer_condensed):
    """
    Decision tree:
    
    1. --prefer-raw flag set?
       YES --> Check RAW exists --> Use RAW (or fallback to CONDENSED)
       NO  --> Continue
    
    2. --prefer-condensed flag set?
       YES --> Check CONDENSED exists --> Use CONDENSED (or fallback to RAW)
       NO  --> Continue
    
    3. No flag set (auto-detect):
       --> Prefer CONDENSED if exists (smaller, faster to process)
       --> Fallback to RAW if CONDENSED doesn't exist
    """
```

### 2.3 Why Auto-Detect Prefers Condensed

When both sources exist, the pipeline defaults to condensed because:
1. **Smaller files** - Faster to process
2. **Cleaner text** - Already filtered and formatted
3. **Consistent quality** - LLM has normalized the prose
4. **Accurate signals** - Keywords in condensed = important keywords

Use `--prefer-raw` when you specifically want raw chapter analysis.

---

## 3. Module Modifications

### 3.1 character_indexing.py Changes

**Before:**
```python
def build_character_index(novel_name, run_id, include_co_occurrences=True):
    chapters_dir = os.path.join(CHAPTERS_CONDENSED_DIR, novel_name)  # HARDCODED
    ...
```

**After:**
```python
def build_character_index(novel_name, run_id, include_co_occurrences=True, 
                          source_dir=None):  # NEW PARAMETER
    if source_dir is None:
        source_dir = CHAPTERS_CONDENSED_DIR  # Default preserves backward compat
    chapters_dir = os.path.join(source_dir, novel_name)
    ...
```

**File detection:**
```python
# Support both .condensed.txt and .txt extensions
chapter_files = sorted([
    f for f in os.listdir(chapters_dir)
    if f.endswith(".condensed.txt") or 
       (f.endswith(".txt") and not f.endswith(".condensed.txt"))
])
```

### 3.2 event_keywords.py Changes

Same pattern as character_indexing:

```python
def build_event_keyword_map(novel_name, run_id, dictionary=KEYWORD_DICTIONARY,
                            source_dir=None):  # NEW PARAMETER
    if source_dir is None:
        source_dir = CHAPTERS_CONDENSED_DIR
    ...
```

### 3.3 Modules That Didn't Need Changes

These modules read from Tier-2/3 JSON artifacts, not chapter files:
- `character_salience.py` - Reads character_index JSON
- `relationship_matrix.py` - Reads character_index + salience JSONs
- `genre_resolver.py` - Reads Tier-3 JSONs
- `tag_resolver.py` - Reads Tier-3 JSONs

---

## 4. Flag Semantics

### 4.1 Inverted Flag Logic

```
run_pipeline.py (condensation-first):
  --character-index     --> ENABLE feature (default: OFF)
  --skip-chapters       --> DISABLE stage (default: ON)

run_analysis_pipeline.py (analysis-first):
  --skip-character-index --> DISABLE feature (default: ON)
  --with-chapters        --> ENABLE stage (default: OFF)
```

### 4.2 AnalysisFlags Dataclass

```python
@dataclass
class AnalysisFlags:
    # Data source (mutually exclusive)
    prefer_raw: bool = False
    prefer_condensed: bool = False
    
    # Analysis features (ON by default)
    skip_character_index: bool = False
    skip_character_salience: bool = False
    skip_relationship_matrix: bool = False
    skip_event_keywords: bool = False
    skip_genre_resolver: bool = False
    skip_tag_resolver: bool = False
    
    # Condensation stages (OFF by default)
    with_chapters: bool = False
    with_arcs: bool = False
    with_novel: bool = False
```

### 4.3 Why "skip_" and "with_" Prefixes?

- `skip_*` prefix makes it clear you're DISABLING something that would otherwise run
- `with_*` prefix makes it clear you're ADDING something that wouldn't otherwise run
- Consistent naming prevents confusion about default behavior

---

## 5. Execution Flow

### 5.1 Analysis Phase (Default)

```
[1] Character Index (Tier-2)
    ├── Reads: {source_dir}/{novel_name}/*.txt
    └── Writes: data/character_index/{novel_name}/{run_id}.character_index.json

[2] Character Salience (Tier-3.1)
    ├── Reads: Character Index JSON
    └── Writes: data/character_salience/{novel_name}/{run_id}.salience.json

[3] Relationship Matrix (Tier-3.2)
    ├── Reads: Character Index + Salience JSONs
    └── Writes: data/relationship_matrix/{novel_name}/{run_id}.relationships.json

[4] Event Keywords (Tier-3.3)
    ├── Reads: {source_dir}/{novel_name}/*.txt
    └── Writes: data/event_keywords/{novel_name}/{run_id}.event_keywords.json

[5] Genre Resolver (Tier-3.4a)
    ├── Reads: Salience + Relationships + Event Keywords JSONs
    └── Writes: data/genre_resolved/{novel_name}/{run_id}.genre_resolved.json

[6] Tag Resolver (Tier-3.4b)
    ├── Reads: All Tier-3 JSONs
    └── Writes: data/tag_resolved/{novel_name}/{run_id}.tag_resolved.json
```

### 5.2 Condensation Phase (Opt-In)

```
[7] Chapter Condensation (if --with-chapters)
    ├── Reads: data/raw/{novel_name}/*.txt
    └── Writes: data/chapters_condensed/{novel_name}/*.condensed.txt

[8] Arc Condensation (if --with-arcs)
    ├── Requires: Chapter condensation complete
    └── Writes: data/arcs_condensed/{novel_name}/*.arc.txt

[9] Novel Condensation (if --with-novel)
    ├── Requires: Arc condensation complete
    └── Writes: data/novel_condensed/{novel_name}/novel.condensed.txt
```

### 5.3 Dependency Validation

```python
# Arc condensation requires chapters
if flags.with_arcs:
    if not flags.with_chapters and not condensed_chapters_exist:
        print("Warning: Arc condensation requires chapter condensation")
        print("Either add --with-chapters or run chapter condensation first")

# Novel condensation requires arcs
if flags.with_novel:
    if not flags.with_arcs and not arc_files_exist:
        print("Warning: Novel condensation requires arc condensation")
```

---

## 6. Use Cases

### 6.1 Quick Metadata Extraction

```bash
# Analyze raw chapters without any condensation
python run_analysis_pipeline.py "My Novel" --prefer-raw
```

**Cost:** Zero LLM calls
**Time:** Seconds (pure text processing)
**Output:** All Tier-2/3 JSON artifacts

### 6.2 Genre Classification Pipeline

```bash
# Just extract genre/tags from existing condensed chapters
python run_analysis_pipeline.py "My Novel" \
  --skip-relationship-matrix \
  --skip-character-salience
```

**Focus:** Character index + Event keywords -> Genre/Tag resolution

### 6.3 Character Network Analysis

```bash
# Full character analysis, no condensation
python run_analysis_pipeline.py "My Novel" \
  --skip-event-keywords \
  --skip-genre-resolver \
  --skip-tag-resolver
```

**Focus:** Character index -> Salience -> Relationships

### 6.4 Full Pipeline (Analysis + Condensation)

```bash
# Everything: analysis first, then condensation
python run_analysis_pipeline.py "My Novel" \
  --with-chapters --with-arcs --with-novel
```

**Equivalent to:** Running all features in `run_pipeline.py`

---

## 7. Comparison with run_pipeline.py

### 7.1 Equivalent Commands

| Goal | run_pipeline.py | run_analysis_pipeline.py |
|------|-----------------|-------------------------|
| All condensation, no analysis | `python run_pipeline.py "Novel"` | `python run_analysis_pipeline.py "Novel" --skip-character-index --skip-character-salience --skip-relationship-matrix --skip-event-keywords --skip-genre-resolver --skip-tag-resolver --with-chapters --with-arcs --with-novel` |
| All analysis, no condensation | `python run_pipeline.py "Novel" --skip-chapters --skip-arcs --skip-novel --character-index --character-salience --relationship-matrix --event-keywords --genre-resolver --tag-resolver` | `python run_analysis_pipeline.py "Novel"` |
| Everything | Add all flags | Add `--with-chapters --with-arcs --with-novel` |

### 7.2 When to Use Which

**Use `run_pipeline.py` when:**
- Primary goal is condensed novel output
- You want condensation to be the default action
- Running in production with known-good workflow

**Use `run_analysis_pipeline.py` when:**
- Primary goal is metadata extraction
- You want to avoid accidental LLM costs
- Experimenting with analysis features
- Working with raw (uncondensed) chapters

---

## 8. Future Considerations

### 8.1 Potential Enhancements

1. **Parallel analysis** - Run independent Tier-3 features concurrently
2. **Selective re-analysis** - Only regenerate specific artifacts
3. **Custom source directories** - Allow arbitrary input paths
4. **Streaming output** - Real-time progress for large novels

### 8.2 API Stability

The `source_dir` parameter additions are **backward compatible**:
- Default value preserves existing behavior
- No changes to existing `run_pipeline.py` calls required
- Can be safely merged without breaking anything

---

## 9. Implementation Checklist

- [x] Create `run_analysis_pipeline.py`
- [x] Add `source_dir` parameter to `character_indexing.py`
- [x] Add `source_dir` parameter to `event_keywords.py`
- [x] Support both `.txt` and `.condensed.txt` file patterns
- [x] Implement data source auto-detection
- [x] Add `--prefer-raw` and `--prefer-condensed` flags
- [x] Add `--skip-*` flags for analysis features
- [x] Add `--with-*` flags for condensation stages
- [x] Update `7. PIPELINE_ORCHESTRATION.md`
- [x] Create `8. ANALYSIS_PIPELINE.md` (this document)
