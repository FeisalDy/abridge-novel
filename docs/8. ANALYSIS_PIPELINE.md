# Analysis-First Pipeline Design

This document describes the **design philosophy** and **implementation details** of the analysis-first orchestrator (`run_analysis_pipeline.py`).

---

## 1. Design Philosophy

### 1.1 The Problem with Condensation-First

The original `run_pipeline.py` assumes:
1. **Condensation is the goal** - You want a shortened novel
2. **Analysis is supplementary** - Tier-2/3 features are "nice to have"
3. **LLM costs are acceptable** - You're willing to pay for condensation

But many use cases DON'T require condensation:
- **Metadata extraction** - Genre/tag classification from existing text
- **Character network analysis** - Understanding relationships without rewriting
- **Keyword scanning** - Finding thematic elements without summarization
- **Prototype testing** - Validating analysis modules before expensive runs

### 1.2 The Analysis-First Paradigm

The analysis-first pipeline inverts the priorities:

```
CONDENSATION-FIRST (run_pipeline.py):
  Condensation (required) --> Analysis (optional)
  "Condense first, analyze if you want"

ANALYSIS-FIRST (run_analysis_pipeline.py):
  Analysis (required) --> Condensation (optional)
  "Analyze first, condense if you need"
```

### 1.3 Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| Analysis ON by default | Primary use case is metadata extraction |
| Condensation OFF by default | Avoid accidental LLM costs |
| RAW chapters supported | Don't require preprocessing |
| Auto-detect data source | Convenient when both exist |
| Non-breaking module changes | Backward compatible with original pipeline |

---

## 2. Data Source Architecture

### 2.1 Dual-Source Support

The analysis pipeline can read from TWO sources:

```
data/
  raw/                    <-- SOURCE 1: Original chapters
    {novel_name}/
      chapter_001.txt
      chapter_002.txt
      ...
  
  chapters_condensed/     <-- SOURCE 2: Condensed chapters
    {novel_name}/
      chapter_001.condensed.txt
      chapter_002.condensed.txt
      ...
```

### 2.2 Source Selection Logic

```python
def determine_data_source(novel_name, prefer_raw, prefer_condensed):
    """
    Decision tree:
    
    1. --prefer-raw flag set?
       YES --> Check RAW exists --> Use RAW (or fallback to CONDENSED)
       NO  --> Continue
    
    2. --prefer-condensed flag set?
       YES --> Check CONDENSED exists --> Use CONDENSED (or fallback to RAW)
       NO  --> Continue
    
    3. No flag set (auto-detect):
       --> Prefer CONDENSED if exists (smaller, faster to process)
       --> Fallback to RAW if CONDENSED doesn't exist
    """
```

### 2.3 Why Auto-Detect Prefers Condensed

When both sources exist, the pipeline defaults to condensed because:
1. **Smaller files** - Faster to process
2. **Cleaner text** - Already filtered and formatted
3. **Consistent quality** - LLM has normalized the prose
4. **Accurate signals** - Keywords in condensed = important keywords

Use `--prefer-raw` when you specifically want raw chapter analysis.

---

## 3. Module Modifications

### 3.1 character_indexing.py Changes

**Before:**
```python
def build_character_index(novel_name, run_id, include_co_occurrences=True):
    chapters_dir = os.path.join(CHAPTERS_CONDENSED_DIR, novel_name)  # HARDCODED
    ...
```

**After:**
```python
def build_character_index(novel_name, run_id, include_co_occurrences=True, 
                          source_dir=None):  # NEW PARAMETER
    if source_dir is None:
        source_dir = CHAPTERS_CONDENSED_DIR  # Default preserves backward compat
    chapters_dir = os.path.join(source_dir, novel_name)
    ...
```

**File detection:**
```python
# Support both .condensed.txt and .txt extensions
chapter_files = sorted([
    f for f in os.listdir(chapters_dir)
    if f.endswith(".condensed.txt") or 
       (f.endswith(".txt") and not f.endswith(".condensed.txt"))
])
```

### 3.2 event_keywords.py Changes

Same pattern as character_indexing:

```python
def build_event_keyword_map(novel_name, run_id, dictionary=KEYWORD_DICTIONARY,
                            source_dir=None):  # NEW PARAMETER
    if source_dir is None:
        source_dir = CHAPTERS_CONDENSED_DIR
    ...
```

### 3.3 Modules That Didn't Need Changes

These modules read from Tier-2/3 JSON artifacts, not chapter files:
- `character_salience.py` - Reads character_index JSON
- `relationship_matrix.py` - Reads character_index + salience JSONs
- `character_profiler.py` - Reads salience + relationships + event_keywords JSONs (Tier-3.3.5)
- `genre_resolver.py` - Reads Tier-3 JSONs + character_profiles
- `tag_resolver.py` - Reads Tier-3 JSONs + character_profiles

---

## 4. Flag Semantics

### 4.1 Inverted Flag Logic

```
run_pipeline.py (condensation-first):
  --character-index     --> ENABLE feature (default: OFF)
  --skip-chapters       --> DISABLE stage (default: ON)

run_analysis_pipeline.py (analysis-first):
  --skip-character-index --> DISABLE feature (default: ON)
  --with-chapters        --> ENABLE stage (default: OFF)
```

### 4.2 AnalysisFlags Dataclass

```python
@dataclass
class AnalysisFlags:
    # Data source (mutually exclusive)
    prefer_raw: bool = False
    prefer_condensed: bool = False
    
    # Analysis features (ON by default)
    skip_character_index: bool = False
    skip_character_salience: bool = False
    skip_relationship_matrix: bool = False
    skip_event_keywords: bool = False
    skip_character_profiles: bool = False  # Tier-3.3.5
    skip_genre_resolver: bool = False
    skip_tag_resolver: bool = False
    
    # Condensation stages (OFF by default)
    with_chapters: bool = False
    with_arcs: bool = False
    with_novel: bool = False
```

### 4.3 Why "skip_" and "with_" Prefixes?

- `skip_*` prefix makes it clear you're DISABLING something that would otherwise run
- `with_*` prefix makes it clear you're ADDING something that wouldn't otherwise run
- Consistent naming prevents confusion about default behavior

---

## 5. Execution Flow

### 5.1 Analysis Phase (Default)

```
[1] Character Index (Tier-2)
    ├── Reads: {source_dir}/{novel_name}/*.txt
    └── Writes: data/character_index/{novel_name}/{run_id}.character_index.json

[2] Character Salience (Tier-3.1)
    ├── Reads: Character Index JSON
    └── Writes: data/character_salience/{novel_name}/{run_id}.character_salience.json

[3] Relationship Matrix (Tier-3.2)
    ├── Reads: Character Index + Salience JSONs
    └── Writes: data/relationship_matrix/{novel_name}/{run_id}.relationship_matrix.json

[4] Event Keywords (Tier-3.3)
    ├── Reads: {source_dir}/{novel_name}/*.txt
    └── Writes: data/event_keywords/{novel_name}/{run_id}.event_keywords.json

[5] Character Profiles (Tier-3.3.5) [NEW]
    ├── Reads: Salience + Relationships + Event Keywords JSONs
    ├── Writes: data/character_profiles/{novel_name}/{run_id}.character_profiles.json
    └── Purpose: Synthesizes raw evidence into per-character state profiles

[6] Genre Resolver (Tier-3.4a)
    ├── Reads: Salience + Relationships + Event Keywords + Character Profiles JSONs
    └── Writes: data/genre_resolved/{novel_name}/{run_id}.genre_resolved.json

[7] Tag Resolver (Tier-3.4b)
    ├── Reads: All Tier-3 JSONs + Character Profiles
    └── Writes: data/tag_resolved/{novel_name}/{run_id}.tag_resolved.json
```

### 5.2 Condensation Phase (Opt-In)

```
[8] Chapter Condensation (if --with-chapters)
    ├── Reads: data/raw/{novel_name}/*.txt
    └── Writes: data/chapters_condensed/{novel_name}/*.condensed.txt

[9] Arc Condensation (if --with-arcs)
    ├── Requires: Chapter condensation complete
    └── Writes: data/arcs_condensed/{novel_name}/*.arc.txt

[10] Novel Condensation (if --with-novel)
    ├── Requires: Arc condensation complete
    └── Writes: data/novel_condensed/{novel_name}/novel.condensed.txt
```

### 5.3 Dependency Validation

```python
# Arc condensation requires chapters
if flags.with_arcs:
    if not flags.with_chapters and not condensed_chapters_exist:
        print("Warning: Arc condensation requires chapter condensation")
        print("Either add --with-chapters or run chapter condensation first")

# Novel condensation requires arcs
if flags.with_novel:
    if not flags.with_arcs and not arc_files_exist:
        print("Warning: Novel condensation requires arc condensation")
```

---

## 6. Use Cases

### 6.1 Quick Metadata Extraction

```bash
# Analyze raw chapters without any condensation
python run_analysis_pipeline.py "My Novel" --prefer-raw
```

**Cost:** Zero LLM calls
**Time:** Seconds (pure text processing)
**Output:** All Tier-2/3 JSON artifacts

### 6.2 Genre Classification Pipeline

```bash
# Just extract genre/tags from existing condensed chapters
python run_analysis_pipeline.py "My Novel" \
  --skip-relationship-matrix \
  --skip-character-salience
```

**Focus:** Character index + Event keywords -> Genre/Tag resolution

### 6.3 Character Network Analysis

```bash
# Full character analysis, no condensation
python run_analysis_pipeline.py "My Novel" \
  --skip-event-keywords \
  --skip-genre-resolver \
  --skip-tag-resolver
```

**Focus:** Character index -> Salience -> Relationships

### 6.4 Full Pipeline (Analysis + Condensation)

```bash
# Everything: analysis first, then condensation
python run_analysis_pipeline.py "My Novel" \
  --with-chapters --with-arcs --with-novel
```

**Equivalent to:** Running all features in `run_pipeline.py`

### 6.5 Character Profile Generation Only

```bash
# Skip resolvers to just generate profiles
python run_analysis_pipeline.py "My Novel" \
  --skip-genre-resolver \
  --skip-tag-resolver
```

**Focus:** Raw evidence synthesis into per-character state profiles

---

## 7. Tier-3.3.5: Character State Profiler

### 7.1 Purpose

The Character State Profiler (`character_profiler.py`) synthesizes raw evidence from upstream tiers into structured per-character profiles. It sits between Event Keywords (Tier-3.3) and Resolvers (Tier-3.4) in the pipeline.

**Key Properties:**
- **Zero LLM calls** - Pure deterministic rule-based logic
- **Evidence synthesis** - Combines signals from salience, relationships, and keywords
- **Profile structure** - Outputs identity, origin, power system, and social attributes
- **Downstream consumer** - Provides structured input for genre/tag resolvers

### 7.2 Input Artifacts

The profiler reads from three upstream JSON artifacts:

| Artifact | Data Used |
|----------|-----------|
| `character_salience.json` | Per-character metrics: mentions, salience, chapter locations |
| `relationship_matrix.json` | Relationship types, strengths, persistence |
| `event_keywords.json` | Keyword detections per chapter |

### 7.3 Output Schema

```json
{
  "novel_name": "Example Novel",
  "run_id": "20250101_120000",
  "profile_version": "1.0.0",
  "profiles": {
    "character_name": {
      "identity": {
        "inferred_gender": "male|female|unknown",
        "evidence_keywords": ["keyword1", "keyword2"],
        "evidence_relationships": ["relationship_type:partner_name"],
        "confidence": 0.85
      },
      "origin_state": {
        "detected_type": "reincarnator|transmigrator|regressor|system_user|native|unknown",
        "temporal_location": "early_story|unknown",
        "evidence_keywords": ["rebirth", "past life"],
        "confidence": 0.9
      },
      "power_system": {
        "detected_system": "cultivation|magic|martial_arts|system|mixed|unknown",
        "evidence_keywords": ["qi", "cultivation"],
        "confidence": 0.75
      },
      "social": {
        "has_harem": true,
        "harem_members": ["Member A", "Member B"],
        "persistent_relationships": ["Ally C", "Mentor D"],
        "evidence": "3+ romantic relationships detected"
      },
      "evidence_summary": {
        "total_mentions": 1500,
        "salience_score": 0.95,
        "chapter_coverage": 0.92,
        "relationship_count": 12
      }
    }
  },
  "generation_timestamp": "2025-01-01T12:00:00"
}
```

### 7.4 Detection Rules

**Gender Inference:**
```python
# Priority order:
1. Romantic relationship roles (husband/wife, boyfriend/girlfriend)
2. Gendered keyword counts (he/him vs she/her)
3. Threshold: 70% majority required
```

**Origin Detection:**
```python
# Keyword pattern matching (protagonist only):
reincarnator_keywords = ["rebirth", "past life", "reincarnation"]
transmigrator_keywords = ["transmigrate", "another world", "isekai"]
regressor_keywords = ["regression", "return in time", "second chance"]
system_user_keywords = ["system", "status window", "level up"]
```

**Power System Detection:**
```python
# Keyword presence in early chapters:
cultivation_keywords = ["qi", "cultivation", "breakthrough", "realm"]
magic_keywords = ["mana", "spell", "magic", "wizard"]
martial_keywords = ["martial arts", "technique", "inner strength"]
```

**Harem Detection:**
```python
# Threshold: 3+ romantic relationships with 50%+ persistence
romantic_types = ["lover", "wife", "husband", "girlfriend", "boyfriend", "fiancé"]
```

### 7.5 Profile Usage in Resolvers

Genre and Tag resolvers can use profile-based conditions:

```python
# Genre resolver example
{
    "evidence_type": "profile_origin_type",
    "check": {"type": "equals", "value": "reincarnator"},
    "weight": 2.0,
    "signal": "Reincarnation protagonist detected"
}

# Tag resolver example
{
    "evidence_type": "profile_protagonist_gender",
    "check": {"type": "equals", "value": "female"},
    "weight": 2.0,
    "signal": "Female protagonist confirmed via profile"
}
```

**Available Profile Conditions:**
| Condition Type | Description |
|----------------|-------------|
| `profile_protagonist_gender` | "male", "female", or "unknown" |
| `profile_origin_type` | "reincarnator", "transmigrator", "regressor", etc. |
| `profile_power_system` | "cultivation", "magic", "martial_arts", etc. |
| `profile_has_harem` | Boolean - protagonist has 3+ romantic partners |

---

## 8. Comparison with run_pipeline.py

### 8.1 Equivalent Commands

| Goal | run_pipeline.py | run_analysis_pipeline.py |
|------|-----------------|-------------------------|
| All condensation, no analysis | `python run_pipeline.py "Novel"` | `python run_analysis_pipeline.py "Novel" --skip-character-index --skip-character-salience --skip-relationship-matrix --skip-event-keywords --skip-genre-resolver --skip-tag-resolver --with-chapters --with-arcs --with-novel` |
| All analysis, no condensation | `python run_pipeline.py "Novel" --skip-chapters --skip-arcs --skip-novel --character-index --character-salience --relationship-matrix --event-keywords --genre-resolver --tag-resolver` | `python run_analysis_pipeline.py "Novel"` |
| Everything | Add all flags | Add `--with-chapters --with-arcs --with-novel` |

### 8.2 When to Use Which

**Use `run_pipeline.py` when:**
- Primary goal is condensed novel output
- You want condensation to be the default action
- Running in production with known-good workflow

**Use `run_analysis_pipeline.py` when:**
- Primary goal is metadata extraction
- You want to avoid accidental LLM costs
- Experimenting with analysis features
- Working with raw (uncondensed) chapters

---

## 9. Future Considerations

### 9.1 Potential Enhancements

1. **Parallel analysis** - Run independent Tier-3 features concurrently
2. **Selective re-analysis** - Only regenerate specific artifacts
3. **Custom source directories** - Allow arbitrary input paths
4. **Streaming output** - Real-time progress for large novels

### 9.2 API Stability

The `source_dir` parameter additions are **backward compatible**:
- Default value preserves existing behavior
- No changes to existing `run_pipeline.py` calls required
- Can be safely merged without breaking anything

---

## 10. Implementation Checklist

- [x] Create `run_analysis_pipeline.py`
- [x] Add `source_dir` parameter to `character_indexing.py`
- [x] Add `source_dir` parameter to `event_keywords.py`
- [x] Support both `.txt` and `.condensed.txt` file patterns
- [x] Implement data source auto-detection
- [x] Add `--prefer-raw` and `--prefer-condensed` flags
- [x] Add `--skip-*` flags for analysis features
- [x] Add `--with-*` flags for condensation stages
- [x] Update `7. PIPELINE_ORCHESTRATION.md`
- [x] Create `8. ANALYSIS_PIPELINE.md` (this document)
- [x] Create `character_profiler.py` (Tier-3.3.5)
- [x] Add `--skip-character-profiles` flag
- [x] Update genre_resolver.py with profile conditions
- [x] Update tag_resolver.py with profile conditions
