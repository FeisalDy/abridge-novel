# Pipeline Orchestration

This document describes the **execution flow** of the Abridge pipeline orchestrators.

There are two orchestrators:
1. **`run_pipeline.py`** - Condensation-first (original, condensation required)
2. **`run_analysis_pipeline.py`** - Analysis-first (NEW, condensation optional)

---

## Table of Contents

1. [Orchestrator Comparison](#1-orchestrator-comparison)
2. [run_pipeline.py (Condensation-First)](#2-run_pipelinepy-condensation-first)
3. [run_analysis_pipeline.py (Analysis-First)](#3-run_analysis_pipelinepy-analysis-first)

---

## 1. Orchestrator Comparison

| Aspect | `run_pipeline.py` | `run_analysis_pipeline.py` |
|--------|-------------------|---------------------------|
| **Philosophy** | Condensation PRIMARY | Analysis PRIMARY |
| **Default behavior** | Runs all 3 condensation stages | Runs all Tier-2/3 analysis |
| **Condensation** | Required (can skip with validation) | Optional (opt-in with flags) |
| **Input source** | Condensed chapters only | RAW or Condensed (auto-detect) |
| **Feature flags** | `--feature` to enable | `--skip-feature` to disable |
| **Use case** | Building condensed artifacts | Extracting structural metadata |

---

# PART 1: run_pipeline.py (Condensation-First)

## 2. run_pipeline.py (Condensation-First)

### 2.1 Overview

The original orchestrator (`run_pipeline.py`) coordinates:
- Three REQUIRED condensation stages (chapter → arc → novel)
- Six OPTIONAL feature stages (Tier-2 and Tier-3)
- Observational instrumentation (guardrails, cost tracking, run reports)

**Entry Point:**
```bash
python run_pipeline.py <novel_name> [flags]
```

### 2.2 Execution Sequence

The pipeline executes in this exact order:

```
1. Initialize run context (run_id, metadata)
2. [Stage 1] Chapter Condensation (or skip if --skip-chapters)
3. [Stage 2] Arc Condensation (or skip if --skip-arcs)
4. [Stage 3] Novel Condensation (or skip if --skip-novel)
5. [Tier-2] Character Index (if --character-index)
6. [Tier-3.1] Character Salience (if --character-salience)
7. [Tier-3.2] Relationship Matrix (if --relationship-matrix)
8. [Tier-3.3] Event Keywords (if --event-keywords)
9. [Tier-3.4a] Genre Resolver (if --genre-resolver)
10. [Tier-3.4b] Tag Resolver (if --tag-resolver)
11. [Finally] Print guardrail summary
12. [Finally] Print cost summary
13. [Finally] Generate and save run report
14. [Finally] End run context
```

**Critical:** Steps 11-14 execute in a `finally` block and run even if earlier stages fail.

---

## 3. Run Context Management

### Run ID Generation

```python
run_id = f"run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
```

Format: `run_YYYYMMDD_HHMMSS_XXXXXXXX`

### Run Metadata

The `RunMetadata` dataclass collects:
- `run_id`: Unique identifier
- `novel_name`: Target novel
- `start_time`: Pipeline start timestamp
- `end_time`: Pipeline end timestamp
- `llm_provider`: Provider name from config
- `model_name`: Model name for provider
- `chapters_skipped`, `arcs_skipped`, `novel_skipped`: Skip status
- `chapters_count`, `arcs_count`: Output counts

---

## 4. Stage Configuration

### Required Directories

| Constant | Value | Purpose |
|----------|-------|---------|
| `RAW_DIR` | `data/raw` | Input chapters |
| `CHAPTERS_CONDENSED_DIR` | `data/chapters_condensed` | Stage 1 output |
| `ARCS_CONDENSED_DIR` | `data/arcs_condensed` | Stage 2 output |
| `NOVEL_CONDENSED_DIR` | `data/novel_condensed` | Stage 3 output |

### SkipFlags Dataclass

```python
@dataclass
class SkipFlags:
    skip_chapters: bool = False
    skip_arcs: bool = False
    skip_novel: bool = False
    character_index: bool = False
    character_salience: bool = False
    relationship_matrix: bool = False
    event_keywords: bool = False
    genre_resolver: bool = False
    tag_resolver: bool = False
```

---

## 5. Skip Validation

Each condensation stage has explicit validation before skip is allowed.

### Chapter Skip Validation (`validate_chapter_outputs`)

**Checks:**
1. Raw directory exists: `data/raw/{novel_name}/`
2. Output directory exists: `data/chapters_condensed/{novel_name}/`
3. Raw chapter count > 0
4. Condensed chapter count > 0
5. **Condensed count == Raw count** (strict 1:1 mapping)

**On validation failure:** Raises `ValueError` with message explaining the issue.

### Arc Skip Validation (`validate_arc_outputs`)

**Checks:**
1. Output directory exists: `data/arcs_condensed/{novel_name}/`
2. At least one arc file present (count > 0)

**Note:** Arc count is not validated against expected count because `CHAPTERS_PER_ARC` may vary.

### Novel Skip Validation (`validate_novel_outputs`)

**Checks:**
1. Output directory exists: `data/novel_condensed/{novel_name}/`
2. File exists: `data/novel_condensed/{novel_name}/novel.condensed.txt`
3. File is non-empty (size > 0)

---

## 6. Stage Execution

### Stage 1: Chapter Condensation

```python
if skip_flags.skip_chapters:
    is_valid, message = validate_chapter_outputs(novel_name)
    if is_valid:
        metadata.chapters_skipped = True
    else:
        raise ValueError(...)
else:
    condense_chapters(novel_name)
```

**Function called:** `chapter_condensation.process_novel(novel_name)`

### Stage 2: Arc Condensation

```python
if skip_flags.skip_arcs:
    is_valid, message = validate_arc_outputs(novel_name)
    if is_valid:
        metadata.arcs_skipped = True
    else:
        raise ValueError(...)
else:
    condense_arcs(novel_name)
```

**Function called:** `arc_condensation.process_novel(novel_name)`

### Stage 3: Novel Condensation

```python
if skip_flags.skip_novel:
    is_valid, message = validate_novel_outputs(novel_name)
    if is_valid:
        metadata.novel_skipped = True
    else:
        raise ValueError(...)
else:
    condense_novel(novel_name)
```

**Function called:** `novel_condensation.process_novel(novel_name)`

---

## 7. Tier-2/3 Feature Execution

Tier-2 and Tier-3 features are OPTIONAL and require explicit flags.

### Tier-2: Character Index

```python
if skip_flags.character_index:
    generate_character_index(novel_name, run_id)
```

**Non-blocking:** Failures logged but do not halt pipeline.

### Tier-3.1: Character Salience

```python
if skip_flags.character_salience:
    generate_salience_index(novel_name, run_id)
```

**Dependency:** Requires Tier-2 character index to exist.

### Tier-3.2: Relationship Matrix

```python
if skip_flags.relationship_matrix:
    generate_relationship_matrix(novel_name, run_id)
```

**Dependencies:** Requires Tier-2 and Tier-3.1 data.

### Tier-3.3: Event Keywords

```python
if skip_flags.event_keywords:
    generate_event_keyword_map(novel_name, run_id)
```

**Dependencies:** None (operates directly on condensed chapters).

### Tier-3.4a: Genre Resolver

```python
if skip_flags.genre_resolver:
    generate_genre_resolved(novel_name, run_id)
```

**Dependencies:** Tier-3.1, Tier-3.2, Tier-3.3 (works best with all three).

### Tier-3.4b: Tag Resolver

```python
if skip_flags.tag_resolver:
    generate_tag_resolved(novel_name, run_id)
```

**Dependencies:** Tier-3.1 through Tier-3.4a.

---

## 8. Observational Instrumentation

### Guardrails

- `start_run()`: Called at pipeline start, returns `run_id`
- `record_condensation()`: Called by each condensation stage
- `print_run_summary(run_id)`: Called in finally block
- `end_run()`: Called in finally block

### Cost Tracking

- `record_llm_usage()`: Called by each LLM invocation
- `print_usage_summary(run_id)`: Called in finally block

### Run Reports

```python
finalized_metadata = finalize_run_metadata()
generate_and_save_report(run_id, novel_name, finalized_metadata)
clear_run_metadata()
```

**Non-blocking:** Report generation errors are logged but do not halt.

---

## 9. Error Handling

### Stage Failures

- LLM failures: Retried up to `MAX_LLM_RETRIES=3` times per unit
- All retries exhausted: `RuntimeError` raised, pipeline halts
- Skip validation failures: `ValueError` raised, pipeline halts

### Tier-2/3 Failures

- Feature generation failures: Logged, pipeline continues
- Missing dependency data: Logged, feature skipped

### Finally Block Guarantee

The following always execute regardless of success/failure:
1. `print_run_summary(run_id)`
2. `print_usage_summary(run_id)`
3. `generate_and_save_report(...)`
4. `clear_run_metadata()`
5. `end_run()`

---

## 10. CLI Reference

```
usage: run_pipeline.py [-h] [--skip-chapters] [--skip-arcs] [--skip-novel]
                       [--character-index] [--character-salience]
                       [--relationship-matrix] [--event-keywords]
                       [--genre-resolver] [--tag-resolver]
                       novel_name

positional arguments:
  novel_name           Name of the novel (subdirectory under data/raw/)

optional arguments:
  --skip-chapters      Skip chapter condensation, reuse existing
  --skip-arcs          Skip arc condensation, reuse existing
  --skip-novel         Skip novel condensation, reuse existing
  --character-index    Generate character surface index (Tier-2)
  --character-salience Compute character salience scores (Tier-3.1)
  --relationship-matrix Compute character pair co-presence signals (Tier-3.2)
  --event-keywords     Scan for event keyword signals (Tier-3.3)
  --genre-resolver     Resolve genres from Tier-3 evidence (Tier-3.4a)
  --tag-resolver       Resolve tags from Tier-3 evidence (Tier-3.4b)
```

---

## 11. Example Invocations

### Full Pipeline (Default)

```bash
python run_pipeline.py "Heaven Reincarnation"
```

Executes: Stages 1, 2, 3 only.

### Resume from Arc Condensation

```bash
python run_pipeline.py "Heaven Reincarnation" --skip-chapters
```

Skips: Stage 1 (requires valid chapter outputs)
Executes: Stages 2, 3

### Full Feature Extraction

```bash
python run_pipeline.py "Heaven Reincarnation" \
  --skip-chapters --skip-arcs --skip-novel \
  --character-index \
  --character-salience \
  --relationship-matrix \
  --event-keywords \
  --genre-resolver \
  --tag-resolver
```

Skips: Stages 1, 2, 3 (requires all valid outputs)
Executes: Tier-2, Tier-3.1, Tier-3.2, Tier-3.3, Tier-3.4a, Tier-3.4b

---

## 12. Idempotency Notes

**Condensation stages:**
- Resume detection skips already-processed units
- Never overwrites existing valid outputs
- Empty outputs treated as corrupted and reprocessed

**Tier-2/3 features:**
- Outputs keyed by `run_id`
- Each run produces new artifacts
- Previous run artifacts are never overwritten
---

# PART 2: run_analysis_pipeline.py (Analysis-First)

## 3. run_analysis_pipeline.py (Analysis-First)

### 3.1 Overview

The analysis-first orchestrator (`run_analysis_pipeline.py`) is designed for:
- **Structural analysis as the primary goal** (Tier-2/3 features)
- **Condensation as optional, opt-in** (not required)
- **Raw OR condensed chapters as input** (auto-detection)

**Philosophy:**
> "What can we learn about a novel's structure WITHOUT expensive LLM condensation?"

**Entry Point:**
```bash
python run_analysis_pipeline.py <novel_name> [flags]
```

### 3.2 Execution Sequence

The analysis pipeline executes in this order:

```
1. Initialize run context (run_id, metadata)
2. [Data Source] Determine input source (RAW or CONDENSED)
3. [Tier-2] Character Index (unless --skip-character-index)
4. [Tier-3.1] Character Salience (unless --skip-character-salience)
5. [Tier-3.2] Relationship Matrix (unless --skip-relationship-matrix)
6. [Tier-3.3] Event Keywords (unless --skip-event-keywords)
7. [Tier-3.4a] Genre Resolver (unless --skip-genre-resolver)
8. [Tier-3.4b] Tag Resolver (unless --skip-tag-resolver)
9. [Optional] Chapter Condensation (if --with-chapters)
10. [Optional] Arc Condensation (if --with-arcs)
11. [Optional] Novel Condensation (if --with-novel)
12. [Finally] Print guardrail summary
13. [Finally] Print cost summary
14. [Finally] Generate and save run report
15. [Finally] End run context
```

**Key Difference:** Steps 3-8 run by DEFAULT. Steps 9-11 are OPT-IN.

### 3.3 Data Source Selection

The pipeline intelligently selects the input source:

```python
def determine_data_source(novel_name, prefer_raw, prefer_condensed):
    """
    Priority:
    1. If --prefer-raw: Use RAW if available, else CONDENSED
    2. If --prefer-condensed: Use CONDENSED if available, else RAW
    3. Default (no flag): Prefer CONDENSED if available, else RAW
    """
```

**Source directories:**
| Source | Directory | File pattern |
|--------|-----------|--------------|
| RAW | `data/raw/{novel_name}/` | `*.txt` |
| CONDENSED | `data/chapters_condensed/{novel_name}/` | `*.condensed.txt` |

**Validation:** At least one chapter file must exist in the selected source.

### 3.4 AnalysisFlags Dataclass

```python
@dataclass
class AnalysisFlags:
    """
    Analysis-first paradigm: features are ON by default.
    Use skip_* flags to disable specific features.
    Use with_* flags to opt into condensation.
    """
    # Data source selection
    prefer_raw: bool = False
    prefer_condensed: bool = False
    
    # Analysis features (ON by default - use skip to disable)
    skip_character_index: bool = False
    skip_character_salience: bool = False
    skip_relationship_matrix: bool = False
    skip_event_keywords: bool = False
    skip_genre_resolver: bool = False
    skip_tag_resolver: bool = False
    
    # Condensation (OFF by default - use with to enable)
    with_chapters: bool = False
    with_arcs: bool = False
    with_novel: bool = False
```

### 3.5 Feature Execution (Default ON)

Unlike `run_pipeline.py`, analysis features run by DEFAULT:

```python
# Tier-2: Character Index
if not flags.skip_character_index:
    generate_character_index(novel_name, run_id, source_dir=source_dir)

# Tier-3.1: Character Salience
if not flags.skip_character_salience:
    generate_salience_index(novel_name, run_id)

# ... etc
```

**Non-blocking:** Feature failures are logged but do not halt the pipeline.

### 3.6 Condensation Execution (Opt-In)

Condensation stages only run when explicitly requested:

```python
# Chapter Condensation (opt-in)
if flags.with_chapters:
    process_novel(novel_name)  # chapter_condensation.process_novel

# Arc Condensation (opt-in)
if flags.with_arcs:
    if flags.with_chapters or condensed_exists:
        process_novel(novel_name)  # arc_condensation.process_novel
    else:
        print("⚠️ Arc condensation requires chapter condensation")

# Novel Condensation (opt-in)
if flags.with_novel:
    if flags.with_arcs or arcs_exist:
        process_novel(novel_name)  # novel_condensation.process_novel
    else:
        print("⚠️ Novel condensation requires arc condensation")
```

### 3.7 CLI Reference

```
usage: run_analysis_pipeline.py [-h]
                                [--prefer-raw] [--prefer-condensed]
                                [--skip-character-index] [--skip-character-salience]
                                [--skip-relationship-matrix] [--skip-event-keywords]
                                [--skip-genre-resolver] [--skip-tag-resolver]
                                [--with-chapters] [--with-arcs] [--with-novel]
                                novel_name

ANALYSIS-FIRST Novel Pipeline

positional arguments:
  novel_name            Name of the novel (subdirectory under data/raw/)

Data Source Selection:
  --prefer-raw          Prefer raw chapters as input (default: auto-detect)
  --prefer-condensed    Prefer condensed chapters as input (default: auto-detect)

Analysis Features (ON by default):
  --skip-character-index     Skip character surface index generation
  --skip-character-salience  Skip character salience computation
  --skip-relationship-matrix Skip relationship matrix computation
  --skip-event-keywords      Skip event keyword scanning
  --skip-genre-resolver      Skip genre resolution
  --skip-tag-resolver        Skip tag resolution

Condensation (OFF by default):
  --with-chapters       Run chapter condensation (requires LLM)
  --with-arcs           Run arc condensation (requires chapters)
  --with-novel          Run novel condensation (requires arcs)
```

### 3.8 Example Invocations

#### Analysis Only (Default)

```bash
python run_analysis_pipeline.py "Heaven Reincarnation"
```

Executes: Tier-2, Tier-3.1 through Tier-3.4b (all analysis)
Skips: All condensation stages
Input: Auto-detects best available source

#### Analysis from Raw Chapters Only

```bash
python run_analysis_pipeline.py "Heaven Reincarnation" --prefer-raw
```

Forces: Use raw chapters even if condensed exist
Useful for: Analyzing untranslated/unprocessed novels

#### Minimal Analysis + Full Condensation

```bash
python run_analysis_pipeline.py "Heaven Reincarnation" \
  --skip-character-salience \
  --skip-relationship-matrix \
  --skip-genre-resolver \
  --skip-tag-resolver \
  --with-chapters --with-arcs --with-novel
```

Analysis: Character Index, Event Keywords only
Condensation: All three stages

#### Skip All Analysis, Just Condense

```bash
python run_analysis_pipeline.py "Heaven Reincarnation" \
  --skip-character-index \
  --skip-character-salience \
  --skip-relationship-matrix \
  --skip-event-keywords \
  --skip-genre-resolver \
  --skip-tag-resolver \
  --with-chapters --with-arcs --with-novel
```

Equivalent to: `run_pipeline.py` (but with inverted semantics)

---

## 4. When to Use Which Orchestrator

| Scenario | Recommended Orchestrator |
|----------|-------------------------|
| Build condensed novel for reading | `run_pipeline.py` |
| Extract metadata from raw chapters | `run_analysis_pipeline.py` |
| Resume interrupted condensation | `run_pipeline.py --skip-*` |
| Experiment with analysis features | `run_analysis_pipeline.py` |
| Production condensation + analysis | `run_pipeline.py --character-index ...` |
| Analyze without LLM costs | `run_analysis_pipeline.py --prefer-raw` |

---

## 5. Module Modifications for Dual-Source Support

To support both orchestrators, the following modules were modified:

### character_indexing.py

```python
def generate_character_index(
    novel_name: str,
    run_id: str,
    include_co_occurrences: bool = True,
    source_dir: Optional[str] = None,  # NEW: defaults to CHAPTERS_CONDENSED_DIR
) -> Optional[str]:
```

### event_keywords.py

```python
def generate_event_keyword_map(
    novel_name: str,
    run_id: str,
    source_dir: Optional[str] = None,  # NEW: defaults to CHAPTERS_CONDENSED_DIR
) -> Optional[str]:
```

**Backward Compatibility:** Both functions default to `CHAPTERS_CONDENSED_DIR`, so existing `run_pipeline.py` calls work unchanged.